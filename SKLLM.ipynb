{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLM + RAG + Agents with Semantic Kernel Memory**\n",
    "This step-by-step exercise will guide you through **Large Language Models (LLMs)**, **Retrieval-Augmented Generation (RAG)**, and **Agents**, using a **local LLama2 model** and **Semantic Kernel (SK) Memory** in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-cpp-python huggingface_hub >nul 2>&1\n",
    "%pip install semantic-kernel >nul 2>&1\n",
    "%pip install ollama >nul 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1: Loading LLama2 and Running a Basic Prompt**\n",
    "### **What You Will Learn**\n",
    "- How to download and load LLama2 locally\n",
    "- How to send a basic query and receive a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Running LLM query...\n",
      "\n",
      "\n",
      "### Model Output ###\n",
      "\n",
      "\n",
      "Large Language Models (LLMs) are AI systems that generate human-like text by learning from vast amounts of data. They can be trained to perform specific tasks, like writing articles or chatting, and can improve over time with more data and training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Ensure all logging is disabled\n",
    "os.environ[\"LLAMA_CPP_LOG_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "MODEL_NAME = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
    "MODEL_FILE = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "# Suppress downloading logs\n",
    "with redirect_stdout(open(os.devnull, \"w\")), redirect_stderr(open(os.devnull, \"w\")):\n",
    "    model_path = hf_hub_download(repo_id=MODEL_NAME, filename=MODEL_FILE)\n",
    "\n",
    "# Suppress model loading logs\n",
    "#with SuppressOutput():\n",
    "llm = Llama(model_path=model_path, n_ctx=4096, verbose=False)\n",
    "\n",
    "# Run the query and print only the needed output\n",
    "print(\"\\nRunning LLM query...\\n\")\n",
    "\n",
    "#with SuppressOutput():\n",
    "response = llm(\"Explain Large Language Models (LLMs) in simple terms. Use 100 Characters.\", max_tokens=128)\n",
    "\n",
    "# Ensure full output is printed without truncation\n",
    "full_output = response['choices'][0]['text']\n",
    "\n",
    "print(\"\\n### Model Output ###\\n\")\n",
    "print(full_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you are running this in a closed environment without internet access, ask your instructor for the model file path.\n",
    "\n",
    "Remove the **, verbose=False** to see the additional logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 2: Implementing Manual RAG Retrieval**\n",
    "### **What You Will Learn**\n",
    "- How to use FAISS for document retrieval\n",
    "- How to create and store vector embeddings\n",
    "- How to retrieve the best-matching fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üîç Best match: The heaviest recorded blue whale weighed approximately 190,000 kg.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision torchaudio >nul 2>&1\n",
    "%pip install faiss-cpu numpy sentence-transformers >nul 2>&1\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample knowledge base with unique facts\n",
    "docs = [\n",
    "    \"RAG improves LLM responses by retrieving relevant external documents.\",\n",
    "    \"Semantic Kernel is an AI orchestration framework.\",\n",
    "    \"LLama2 is a local, open-source language model.\",\n",
    "    \"The Moon has a diameter of 3,474 km.\",\n",
    "    \"The heaviest recorded blue whale weighed approximately 190,000 kg.\",\n",
    "    \"The Eiffel Tower can be 15 cm taller in the summer due to thermal expansion.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "doc_embeddings = np.array([embedder.encode(doc) for doc in docs])\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"vector_store.index\")\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is the weight of the heaviest blue whale?\"\n",
    "query_embedding = embedder.encode(query).reshape(1, -1)\n",
    "D, I = index.search(query_embedding, k=1)\n",
    "retrieved_doc = docs[I[0][0]]\n",
    "\n",
    "print(f'üîç Best match: {retrieved_doc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 3: Enhancing LLM Responses with Retrieved Data**\n",
    "### **What You Will Learn**\n",
    "- How to retrieve relevant knowledge before generating a response\n",
    "- How to use the retrieved data as context in a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Best match: The heaviest recorded blue whale weighed approximately 190,000 kg.\n",
      "\n",
      "\n",
      "### Model Output ###\n",
      "\n",
      "  Based on the information retrieved from the knowledge base, the heaviest blue whale weighs approximately 190,000 kg.\n"
     ]
    }
   ],
   "source": [
    "# Query for a specific fact\n",
    "query = \"How much does the heaviest blue whale weigh?\"\n",
    "query_embedding = embedder.encode(query).reshape(1, -1)\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"vector_store.index\")\n",
    "\n",
    "# Perform search\n",
    "D, I = index.search(query_embedding, k=1)\n",
    "retrieved_doc = docs[I[0][0]]\n",
    "\n",
    "# Display retrieved knowledge\n",
    "print(f\"\\nüîç Best match: {retrieved_doc}\\n\")\n",
    "\n",
    "# Use retrieved document in LLM prompt\n",
    "context = retrieved_doc\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are an AI assistant that enhances responses using retrieved knowledge.\n",
    "Below is relevant information from a knowledge base:\n",
    "{context}\n",
    "<</SYS>>\n",
    "Answer the question: {query}[/INST]\"\"\"\n",
    "\n",
    "# Generate response\n",
    "response = llm(prompt, max_tokens=160)\n",
    "    \n",
    "print(\"\\n### Model Output ###\\n\")\n",
    "print(response['choices'][0]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 4: Using Semantic Kernel Memory for Retrieval**\n",
    "### **What You Will Learn**\n",
    "- How to store and retrieve information using Semantic Kernel Memory\n",
    "- How to replace manual FAISS retrieval with SK Memory\n",
    "- How to integrate memory-based retrieval into LLM prompts\n",
    "- How to abstract the model for both the chat completion and the memory (RAG) with Semantic Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Retrieved from SK Memory: The heaviest recorded blue whale weighed approximately 190,000 kg.\n",
      "\n",
      "\n",
      "### Model Output ###\n",
      "\n",
      "  Ah, an excellent question! According to my retrieved knowledge, the heaviest blue whale ever recorded weighed approximately 190,000 kilograms (or 190 tons). That's a whopping amount of weight! Can I help you with anything else?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.memory import VolatileMemoryStore\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "\n",
    "class LocalLlamaTextCompletion:\n",
    "    def __init__(self, llama_instance, service_id: str):\n",
    "        self.llama = llama_instance\n",
    "        self.service_id = service_id\n",
    "\n",
    "    async def complete(self, prompt: str, max_tokens: int = 200) -> str:\n",
    "        response = await asyncio.to_thread(self.llama, prompt, max_tokens=max_tokens)\n",
    "        return response['choices'][0]['text']\n",
    "\n",
    "\n",
    "# Initialize our local completion service.\n",
    "local_llama_completion = LocalLlamaTextCompletion(llm, \"local_llama\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Set up the local embedding service for the memory store.\n",
    "# ---------------------------\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
    "\n",
    "def generate_embedding(text: str):\n",
    "    # Tokenize the input text.\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        model_output = embed_model(**inputs)\n",
    "    # Mean pooling to get a fixed-size embedding vector.\n",
    "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "class LocalEmbeddingService:\n",
    "    async def generate_embeddings(self, texts, **kwargs):\n",
    "        embeddings = []\n",
    "        for t in texts:\n",
    "            emb = await asyncio.to_thread(generate_embedding, t)\n",
    "            embeddings.append(emb)\n",
    "        return embeddings\n",
    "\n",
    "local_embedding_service = LocalEmbeddingService()\n",
    "\n",
    "# ---------------------------\n",
    "# Initialize Semantic Kernel memory (SK Memory) with a volatile store.\n",
    "# ---------------------------\n",
    "memory_store = VolatileMemoryStore()\n",
    "sk_memory = SemanticTextMemory(memory_store, local_embedding_service)\n",
    "\n",
    "# ---------------------------\n",
    "# Set up the Semantic Kernel and register the local completion service.\n",
    "# ---------------------------\n",
    "kernel = Kernel()\n",
    "# Register the local LLaMA completion service under a service id.\n",
    "# When creating a semantic function, we will reference it by \"local_llama\".\n",
    "kernel.add_service(local_llama_completion)\n",
    "\n",
    "# ---------------------------\n",
    "# Populate your knowledge base.\n",
    "# ---------------------------\n",
    "knowledge_base = {\n",
    "    \"fact_rag\": \"RAG improves LLM responses by retrieving relevant external documents.\",\n",
    "    \"fact_sk\": \"Semantic Kernel is an AI orchestration framework.\",\n",
    "    \"fact_llama2\": \"LLama2 is a local, open-source language model.\",\n",
    "    \"fact_moon\": \"The Moon has a diameter of 3,474 km.\",\n",
    "    \"fact_bluewhale\": \"The heaviest recorded blue whale weighed approximately 190,000 kg.\",\n",
    "    \"fact_eiffel\": \"The Eiffel Tower can be 15 cm taller in the summer due to thermal expansion.\"\n",
    "}\n",
    "\n",
    "async def save_knowledge():\n",
    "    for fact_id, fact_text in knowledge_base.items():\n",
    "        await sk_memory.save_information(collection=\"knowledge_base\", text=fact_text, id=fact_id)\n",
    "\n",
    "# Save the facts to memory.\n",
    "await save_knowledge()\n",
    "\n",
    "# Define your query.\n",
    "query = \"How much does the heaviest blue whale weigh?\"\n",
    "\n",
    "# Perform a semantic search using the local embedding service.\n",
    "results = await sk_memory.search(collection=\"knowledge_base\", query=query, limit=1, min_relevance_score=0.0)\n",
    "if not results:\n",
    "    print(\"No results found.\")\n",
    "    exit()\n",
    "\n",
    "retrieved_info = results[0]\n",
    "print(f\"\\nüß† Retrieved from SK Memory: {retrieved_info.text}\\n\")\n",
    "\n",
    "# Define the prompt template. Notice that we include placeholders for the retrieved info and the query.\n",
    "prompt_template = (\n",
    "    \"[INST] <<SYS>>\\n\"\n",
    "    \"You are an AI assistant using retrieved knowledge.\\n\"\n",
    "    \"Relevant Info: {retrieved_info}\\n\"\n",
    "    \"<</SYS>>\\n\"\n",
    "    \"Answer the question: {query} [/INST]\"\n",
    ")\n",
    "formatted_prompt = prompt_template.format(\n",
    "    retrieved_info=retrieved_info.text,\n",
    "    query=query\n",
    ")\n",
    "result = await kernel.services[\"local_llama\"].complete(formatted_prompt, max_tokens=200)\n",
    "print(\"\\n### Model Output ###\\n\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 5: Using Semantic Kernel Functions to provide information**\n",
    "Since Semantic Kernel can call a function using instruct models, we are going to use a local OLlama server\n",
    "\n",
    "### **What You Will Learn**\n",
    "- Running and playing with a local Ollama server\n",
    "- Loading a model and chat with it\n",
    "- Write a code that:\n",
    "    - Add memory access using Semantic Kernel plugin function\n",
    "    - Add a function that allow the Kernel to calculate\n",
    "\n",
    "\n",
    "### Using Ollama local server\n",
    "\n",
    "```\n",
    "docker run -d --name ollama -p 11434:11434 ollama/ollama:latest\n",
    "```\n",
    "#### Start the server\n",
    "\n",
    "```\n",
    "docker exec -it ollama ollama serve\n",
    "```\n",
    "\n",
    "#### Chat with a model\n",
    "\n",
    "```\n",
    "docker exec -it ollama ollama run llama3.2\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:> To convert this to pounds, we can multiply by 2.20462 (since there are 2.20462 pounds in a kilogram):\n",
      "\n",
      "189,000 kg * 2.20462 lbs/kg ‚âà 416,000 pounds.\n",
      "\n",
      "So, the heaviest recorded blue whale weighed approximately 416,000 pounds.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.memory import VolatileMemoryStore\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "from semantic_kernel.core_plugins.math_plugin import MathPlugin\n",
    "from semantic_kernel.core_plugins.time_plugin import TimePlugin\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Initialize memory store and semantic text memory\n",
    "memory_store = VolatileMemoryStore()\n",
    "\n",
    "# Set up the local embedding service for the memory store\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
    "\n",
    "def generate_embedding(text: str):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        model_output = embed_model(**inputs)\n",
    "    # Mean pooling to get a fixed-size embedding vector\n",
    "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "class LocalEmbeddingService:\n",
    "    async def generate_embeddings(self, texts, **kwargs):\n",
    "        embeddings = []\n",
    "        for t in texts:\n",
    "            emb = await asyncio.to_thread(generate_embedding, t)\n",
    "            embeddings.append(emb)\n",
    "        return embeddings\n",
    "\n",
    "local_embedding_service = LocalEmbeddingService()\n",
    "sk_memory = SemanticTextMemory(memory_store, local_embedding_service)\n",
    "\n",
    "#\n",
    "# Add plugins to the kernel\n",
    "kernel.add_plugin(MathPlugin(), plugin_name=\"math\")\n",
    "kernel.add_plugin(TimePlugin(), plugin_name=\"time\")\n",
    "\n",
    "# Knowledge base with facts\n",
    "# --- SK Memory Setup ---\n",
    "memory_store = VolatileMemoryStore()\n",
    "sk_memory = SemanticTextMemory(memory_store, local_embedding_service)\n",
    "\n",
    "# --- Populate Knowledge Base ---\n",
    "knowledge_base = {\n",
    "    \"fact_rag\": \"RAG improves LLM responses by retrieving relevant external documents.\",\n",
    "    \"fact_sk\": \"Semantic Kernel is an AI orchestration framework.\",\n",
    "    \"fact_llama2\": \"LLama2 is a local, open-source language model.\",\n",
    "    \"fact_moon\": \"The Moon has a diameter of 3,474 km.\",\n",
    "    \"fact_bluewhale\": \"The heaviest recorded blue whale weighed approximately 190,000 kg.\",\n",
    "    \"fact_eiffel\": \"The Eiffel Tower can be 15 cm taller in the summer due to thermal expansion.\"\n",
    "}\n",
    "\n",
    "async def save_knowledge():\n",
    "    for fact_id, fact_text in knowledge_base.items():\n",
    "        await sk_memory.save_information(collection=\"knowledge_base\", text=fact_text, id=fact_id)\n",
    "\n",
    "# Save the facts to memory\n",
    "await save_knowledge()\n",
    "\n",
    "class MemoryPlugin:\n",
    "    def __init__(self, memory):\n",
    "        self.memory = memory\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"retrieve_fact\",\n",
    "        description=\"Retrieve a fact from memory based on the user's query using RAG. \"\n",
    "                    \"The fact returned will contain a weight in kilograms embedded in text.\"\n",
    "    )\n",
    "    async def retrieve_fact(self, query: str) -> str:\n",
    "        \"\"\"Retrieve a fact from SK Memory using RAG.\"\"\"\n",
    "        results = await self.memory.search(collection=\"knowledge_base\", query=query, limit=1, min_relevance_score=0.0)\n",
    "        return results[0].text if results else \"No relevant fact found.\"\n",
    "\n",
    "kernel.add_plugin(MemoryPlugin(sk_memory), \"memory_plugin\")\n",
    "\n",
    "# Configure the Ollama chat completion service\n",
    "model_name = \"llama3.2\"  # Ensure this model is pulled and available\n",
    "ollama_endpoint = \"http://localhost:11434\"\n",
    "chat_completion_service = OllamaChatCompletion(ai_model_id=model_name, host=ollama_endpoint)\n",
    "\n",
    "# Create request settings for Ollama\n",
    "request_settings = OllamaChatPromptExecutionSettings()\n",
    "request_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={\"excluded_plugins\": [\"ChatBot\"]})\n",
    "\n",
    "# Register the Ollama service with the kernel\n",
    "kernel.add_service(chat_completion_service)\n",
    "\n",
    "# User query\n",
    "user_input = \"How much does the heaviest blue whale weigh in pounds?\"\n",
    "\n",
    "# Initialize chat history\n",
    "history = ChatHistory()\n",
    "history.add_user_message(user_input)\n",
    "\n",
    "# Update arguments with user input and chat history\n",
    "arguments = KernelArguments(settings=request_settings)\n",
    "arguments[\"user_input\"] = user_input\n",
    "arguments[\"chat_history\"] = history\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\")\n",
    "    \n",
    "# Invoke the chat function\n",
    "result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "\n",
    "# Process the result\n",
    "if result:\n",
    "    response = result.value[0]\n",
    "    print(f\"Chatbot:> {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Exercise**\n",
    "\n",
    "**Exercise 6: Building a Disk Traversal and Summarization Chatbot**\n",
    "\n",
    "**Objective:**\n",
    "Develop a Python application that traverses a specified directory, identifies text-based documents, generates concise summaries for each, stores these summaries in memory, and enables interactive querying of this information through a chatbot interface.\n",
    "\n",
    "**Key Learning Outcomes:**\n",
    "- Implementing directory traversal to locate text documents.\n",
    "- Applying text summarization techniques to distill essential information.\n",
    "- Storing and managing summaries using Semantic Kernel Memory.\n",
    "- Creating an interactive chatbot capable of responding to user queries based on the stored summaries.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Directory Traversal:**\n",
    "   - Utilize Python's `os` or `pathlib` modules to recursively traverse a user-specified directory.\n",
    "   - Identify and collect paths of text-based documents (e.g., `.txt`, `.md`, `.docx`, `.pdf`).\n",
    "   - For simplicity target `.txt`, `.md`\n",
    "   - You can use Python libraries to target `.docx`, `.pdf`\n",
    "\n",
    "2. **Storing Summaries in Semantic Kernel Memory:**\n",
    "   - Integrate Semantic Kernel (SK) Memory to store the generated summaries.\n",
    "\n",
    "3. **Developing the Chatbot Interface:**\n",
    "   - Create an interactive chatbot that can handle user queries related to the summarized documents.\n",
    "   - Upon receiving a query, the chatbot should:\n",
    "     - Retrieve relevant summaries from SK Memory.\n",
    "     - Provide coherent and contextually relevant responses based on the stored information.\n",
    "\n",
    "This exercise aims to consolidate your understanding of integrating file system operations, text processing, memory management, and interactive user interfaces within a cohesive application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up offline environment**\n",
    "\n",
    "To set up your environment for running the provided Jupyter Notebook in a disconnected setting, follow these steps:\n",
    "\n",
    "1. **Download and Prepare Dependencies**: Use the following script to download all necessary models and Docker images. This script should be executed in an environment with internet access.\n",
    "\n",
    "   ```bash\n",
    "   #!/bin/bash\n",
    "\n",
    "   # Create a directory to store all resources\n",
    "   mkdir -p llm_resources\n",
    "   cd llm_resources\n",
    "\n",
    "   # Download the LLaMA 2 model\n",
    "   git lfs install\n",
    "   git clone https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "   mv Llama-2-7B-Chat-GGUF models\n",
    "\n",
    "   # Pull the Ollama Docker image\n",
    "   docker pull ollama/ollama:latest\n",
    "   docker save ollama/ollama:latest -o ollama_latest.tar\n",
    "\n",
    "   # Create a requirements file for Python dependencies\n",
    "   cat <<EOF > requirements.txt\n",
    "   torch\n",
    "   torchvision\n",
    "   torchaudio\n",
    "   faiss-cpu\n",
    "   numpy\n",
    "   sentence-transformers\n",
    "   semantic-kernel\n",
    "   huggingface_hub\n",
    "   llama-cpp-python\n",
    "   EOF\n",
    "\n",
    "   # Download Python packages\n",
    "   pip download -r requirements.txt -d python_packages\n",
    "\n",
    "   echo \"All resources have been downloaded and saved in the 'llm_resources' directory.\"\n",
    "   ```\n",
    "\n",
    "\n",
    "   **Instructions**:\n",
    "\n",
    "   - Run the above script on a machine with internet access.\n",
    "   - Transfer the `llm_resources` directory to your target offline environment.\n",
    "\n",
    "2. **Set Up in the Disconnected Environment**:\n",
    "\n",
    "   - **Install Docker**: Ensure Docker is installed on your offline machine. If not, download the Docker installation package appropriate for your system and transfer it to the machine for installation.\n",
    "\n",
    "   - **Load the Ollama Docker Image**: Navigate to the `llm_resources` directory and load the Docker image:\n",
    "\n",
    "     ```bash\n",
    "     docker load -i ollama_latest.tar\n",
    "     ```\n",
    "\n",
    "   - **Install Python Dependencies**: Use the pre-downloaded Python packages to set up your environment:\n",
    "\n",
    "     ```bash\n",
    "     pip install --no-index --find-links=python_packages -r requirements.txt\n",
    "     ```\n",
    "\n",
    "   - **Set Up Models**: Ensure that the downloaded LLaMA 2 model is placed in the appropriate directory as expected by your Jupyter Notebook.\n",
    "\n",
    "3. **Running the Jupyter Notebook**:\n",
    "\n",
    "   - **Start the Ollama Server**: Run the Ollama server using Docker:\n",
    "\n",
    "     ```bash\n",
    "     docker run -d --name ollama -p 11434:11434 ollama/ollama:latest\n",
    "     ```\n",
    "\n",
    "   - **Launch Jupyter Notebook**: Navigate to your project directory and start Jupyter Notebook:\n",
    "\n",
    "     ```bash\n",
    "     jupyter notebook\n",
    "     ```\n",
    "\n",
    "   - **Access the Notebook**: Open your web browser and navigate to the Jupyter Notebook interface to open and run your notebook.\n",
    "\n",
    "By following these steps, you can set up and run your Jupyter Notebook in an environment without internet access. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
