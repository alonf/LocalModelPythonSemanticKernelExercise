{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLM + Function Call with Semantic Kernel Memory**\n",
    "This will guide you through **Large Language Models (LLMs)**, and **Semantic Kernel Plugin** in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install semantic-kernel >nul 2>&1\n",
    "% pip install ollama >nul 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1: Running a Basic Prompt**\n",
    "### **What You Will Learn**\n",
    "- How to send a basic query and receive a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:> The capital of Israel is Jerusalem.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Configure the Ollama chat completion service\n",
    "model_name = \"llama3.2\"  # Ensure this model is pulled and available\n",
    "ollama_endpoint = \"http://localhost:11434\"\n",
    "chat_completion_service = OllamaChatCompletion(ai_model_id=model_name, host=ollama_endpoint)\n",
    "\n",
    "# Create request settings for Ollama\n",
    "request_settings = OllamaChatPromptExecutionSettings()\n",
    "\n",
    "# Register the Ollama service with the kernel\n",
    "kernel.add_service(chat_completion_service)\n",
    "\n",
    "# User query\n",
    "user_input = \"What is the capital of Israel?\"\n",
    "\n",
    "\n",
    "\n",
    "# Invoke the chat function\n",
    "result = await kernel.invoke_prompt(user_input)\n",
    "\n",
    "# Process the result\n",
    "if result:\n",
    "    response = result.value[0]\n",
    "    print(f\"Chatbot:> {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 2: Using Time Plugin to Provide Time Information**\n",
    "\n",
    "In this exercise, you will:\n",
    "- Utilize the TimePlugin to retrieve and display the current time.\n",
    "- Integrate the plugin with the Semantic Kernel to handle time-related queries.\n",
    "- Create a function that responds to user input asking for the current time.\n",
    "- Test your implementation by sending queries like \"What time is it now?\" and verifying that the chatbot returns the correct time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:> The current time is Sunday, February 23, 2025 10:21 PM.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.core_plugins.math_plugin import MathPlugin\n",
    "from semantic_kernel.core_plugins.time_plugin import TimePlugin\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Configure the Ollama chat completion service\n",
    "model_name = \"llama3.2\"  # Ensure this model is pulled and available\n",
    "ollama_endpoint = \"http://localhost:11434\"\n",
    "chat_completion_service = OllamaChatCompletion(ai_model_id=model_name, host=ollama_endpoint)\n",
    "\n",
    "# Create request settings for Ollama\n",
    "request_settings = OllamaChatPromptExecutionSettings()\n",
    "request_settings = OllamaChatPromptExecutionSettings()\n",
    "request_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={\"excluded_plugins\": [\"ChatBot\"]})\n",
    "\n",
    "# Register the Ollama service with the kernel\n",
    "kernel.add_service(chat_completion_service)\n",
    "kernel.add_plugin(TimePlugin(), plugin_name=\"time\")\n",
    "\n",
    "# User query\n",
    "user_input = \"What time is it now?\"\n",
    "\n",
    "# Initialize chat history\n",
    "history = ChatHistory()\n",
    "history.add_user_message(user_input)\n",
    "\n",
    "# Update arguments with user input and chat history\n",
    "arguments = KernelArguments(settings=request_settings)\n",
    "arguments[\"user_input\"] = user_input\n",
    "arguments[\"chat_history\"] = history\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\")\n",
    "    \n",
    "# Invoke the chat function\n",
    "result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "\n",
    "# Process the result\n",
    "if result:\n",
    "    response = result.value[0]\n",
    "    print(f\"Chatbot:> {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 3: Using Semantic Kernel Functions to provide information**\n",
    "Since Semantic Kernel can call a function using instruct models, we are going to use a local OLlama server\n",
    "\n",
    "### **What You Will Learn**\n",
    "- Running and playing with a local Ollama server\n",
    "- Loading a model and chat with it\n",
    "- Write a code that:\n",
    "    - Add a plugin and a function that allow the Kernel to get external information\n",
    "\n",
    "\n",
    "### Using Ollama local server with docker\n",
    "\n",
    "```\n",
    "docker run -d --name ollama -p 11434:11434 ollama/ollama:latest\n",
    "```\n",
    "#### Start the server\n",
    "\n",
    "```\n",
    "docker exec -it ollama ollama serve\n",
    "```\n",
    "\n",
    "#### Chat with a model\n",
    "\n",
    "```\n",
    "docker exec -it ollama ollama run llama3.2\n",
    "\n",
    "```\n",
    "\n",
    "### Using Ollama local server without docker\n",
    "\n",
    "```\n",
    "ollama serve\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:> Here's a summary of the operating system information:\n",
      "\n",
      "**Load Average:** The system is currently running with an average load of 2.10, indicating moderate usage.\n",
      "\n",
      "**Memory Info:**\n",
      "\n",
      "* Total memory: 32.8 GB\n",
      "* Free memory: 25.6 GB\n",
      "* Available memory: 28.1 GB\n",
      "* Buffers: 2.8 MB\n",
      "* Caches: 2.3 GB\n",
      "\n",
      "**Network Devices:**\n",
      "\n",
      "* The system is currently unable to read information about the network device \"\\\\wsl.localhost\\\\Ubuntu-20.04\\\\proc\\\\net\\\\dev\" due to an invalid argument.\n",
      "\n",
      "**Uptime:** The system has been running for approximately 952 minutes (or about 15 hours and 52 minutes) or 41580 seconds, with a second value indicating an extended uptime period of 32 hours and 3 minutes.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.core_plugins.math_plugin import MathPlugin\n",
    "from semantic_kernel.core_plugins.time_plugin import TimePlugin\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "class ProcInfoPlugin:\n",
    "    @kernel_function(\n",
    "        name=\"read_proc_info\",\n",
    "        description=\"Provides information about the operating system, including load average, memory info, network devices, and uptime.\"\n",
    "    )\n",
    "    async def read_proc_info(self) -> str:\n",
    "        \"\"\"Reads system information from /proc and returns a formatted summary.\"\"\"\n",
    "\n",
    "        async def read_file(path: str) -> str:\n",
    "            \"\"\"Asynchronously reads a file, handling errors gracefully.\"\"\"\n",
    "            try:\n",
    "                return await asyncio.to_thread(lambda: open(path, \"r\").read().strip())\n",
    "            except FileNotFoundError:\n",
    "                return f\"Error: {path} not found.\"\n",
    "            except PermissionError:\n",
    "                return f\"Error: Permission denied for {path}.\"\n",
    "            except Exception as e:\n",
    "                return f\"Error reading {path}: {e}\"\n",
    "\n",
    "        #For Windows WSL - Check the your specific path\n",
    "        BASE_PATH =  r\"\\\\wsl.localhost\\Ubuntu-20.04\\proc\"\n",
    "\n",
    "        #For Linux\n",
    "        #BASE_PATH = \"/proc\"\n",
    "\n",
    "        # Read system information files asynchronously\n",
    "        loadavg, meminfo, netdev, uptime = await asyncio.gather(\n",
    "            read_file(os.path.join(BASE_PATH, \"loadavg\")),\n",
    "            read_file(os.path.join(BASE_PATH, \"meminfo\")),\n",
    "            read_file(os.path.join(BASE_PATH, \"net/dev\")),\n",
    "            read_file(os.path.join(BASE_PATH, \"uptime\")),\n",
    "        )\n",
    "\n",
    "        # Format output for readability\n",
    "        formatted_meminfo = \"\\n\".join(meminfo.splitlines()[:10])  # Show first 10 lines\n",
    "        formatted_netdev = \"\\n\".join(netdev.splitlines()[:5])  # Show first 5 lines\n",
    "\n",
    "        return (\n",
    "            f\"=== System Information ===\\n\\n\"\n",
    "            f\"ðŸ“Š **Load Average:**\\n{loadavg}\\n\\n\"\n",
    "            f\"ðŸ›‘ **Memory Info (First 10 Lines):**\\n{formatted_meminfo}\\n\\n\"\n",
    "            f\"ðŸŒ **Network Devices (First 5 Lines):**\\n{formatted_netdev}\\n\\n\"\n",
    "            f\"â³ **Uptime:** {uptime} seconds\"\n",
    "        )\n",
    "\n",
    "kernel.add_plugin(ProcInfoPlugin(), \"proc_info_plugin\")\n",
    "\n",
    "# Configure the Ollama chat completion service\n",
    "model_name = \"llama3.2\"  # Ensure this model is pulled and available\n",
    "ollama_endpoint = \"http://localhost:11434\"\n",
    "chat_completion_service = OllamaChatCompletion(ai_model_id=model_name, host=ollama_endpoint)\n",
    "\n",
    "# Create request settings for Ollama\n",
    "request_settings = OllamaChatPromptExecutionSettings()\n",
    "request_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={\"excluded_plugins\": [\"ChatBot\"]})\n",
    "\n",
    "# Register the Ollama service with the kernel\n",
    "kernel.add_service(chat_completion_service)\n",
    "\n",
    "# User query\n",
    "user_input = \"Provide a summery information about the operating system information\"\n",
    "\n",
    "# Initialize chat history\n",
    "history = ChatHistory()\n",
    "history.add_user_message(user_input)\n",
    "\n",
    "# Update arguments with user input and chat history\n",
    "arguments = KernelArguments(settings=request_settings)\n",
    "arguments[\"user_input\"] = user_input\n",
    "arguments[\"chat_history\"] = history\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\")\n",
    "    \n",
    "# Invoke the chat function\n",
    "result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "\n",
    "# Process the result\n",
    "if result:\n",
    "    response = result.value[0]\n",
    "    print(f\"Chatbot:> {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 4: Using Semantic Kernel Functions to take action**\n",
    "\n",
    "### **What You Will Learn**\n",
    "- Write a code that create a file using prompt instructions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:> Here is the system information written to the file C:\\temp\\system_info.txt:\n",
      "\n",
      "C:\\temp\\System Info (System Information).txt\n",
      "==============================================\n",
      "\n",
      "Load Average:\n",
      "8.27 5.64 4.13 1/823 3954\n",
      "\n",
      "\n",
      "Memory Info:\n",
      "MemTotal:       32811156 kB\n",
      "MemFree:        25958100 kB\n",
      "MemAvailable:   27986108 kB\n",
      "Buffers:            3104 kB\n",
      "Cached:          2382792 kB\n",
      "SwapCached:            0 kB\n",
      "Active:          2005060 kB\n",
      "Inactive:        4136884 kB\n",
      "Active(anon):      16104 kB\n",
      "Inactive(anon):  3772284 kB\n",
      "\n",
      "\n",
      "Network Devices:\n",
      "Error reading \\\\wsl.localhost\\Ubuntu-20.04\\proc\\net/dev: [Errno 22] Invalid argument: '\\\\\\\\wsl.localhost\\\\Ubuntu-20.04\\\\proc\\\\net/dev'\n",
      "\n",
      "\n",
      "Uptime:\n",
      "95910.41 seconds\n",
      "4184314.81 seconds\n",
      "\n",
      "The system information has been written to the file C:\\temp\\System Info (System Information).txt\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.core_plugins.math_plugin import MathPlugin\n",
    "from semantic_kernel.core_plugins.time_plugin import TimePlugin\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "class ProcInfoPlugin:\n",
    "    @kernel_function(\n",
    "        name=\"read_proc_info\",\n",
    "        description=\"Provides information about the operating system, including load average, memory info, network devices, and uptime.\"\n",
    "    )\n",
    "    async def read_proc_info(self) -> str:\n",
    "        \"\"\"Reads system information from /proc and returns a formatted summary.\"\"\"\n",
    "\n",
    "        async def read_file(path: str) -> str:\n",
    "            \"\"\"Asynchronously reads a file, handling errors gracefully.\"\"\"\n",
    "            try:\n",
    "                return await asyncio.to_thread(lambda: open(path, \"r\").read().strip())\n",
    "            except FileNotFoundError:\n",
    "                return f\"Error: {path} not found.\"\n",
    "            except PermissionError:\n",
    "                return f\"Error: Permission denied for {path}.\"\n",
    "            except Exception as e:\n",
    "                return f\"Error reading {path}: {e}\"\n",
    "\n",
    "        #For Windows WSL - Check the your specific path\n",
    "        BASE_PATH =  r\"\\\\wsl.localhost\\Ubuntu-20.04\\proc\"\n",
    "\n",
    "        #For Linux\n",
    "        #BASE_PATH = \"/proc\"\n",
    "\n",
    "        # Read system information files asynchronously\n",
    "        loadavg, meminfo, netdev, uptime = await asyncio.gather(\n",
    "            read_file(os.path.join(BASE_PATH, \"loadavg\")),\n",
    "            read_file(os.path.join(BASE_PATH, \"meminfo\")),\n",
    "            read_file(os.path.join(BASE_PATH, \"net/dev\")),\n",
    "            read_file(os.path.join(BASE_PATH, \"uptime\")),\n",
    "        )\n",
    "\n",
    "        # Format output for readability\n",
    "        formatted_meminfo = \"\\n\".join(meminfo.splitlines()[:10])  # Show first 10 lines\n",
    "        formatted_netdev = \"\\n\".join(netdev.splitlines()[:5])  # Show first 5 lines\n",
    "\n",
    "        return (\n",
    "            f\"=== System Information ===\\n\\n\"\n",
    "            f\"ðŸ“Š **Load Average:**\\n{loadavg}\\n\\n\"\n",
    "            f\"ðŸ›‘ **Memory Info (First 10 Lines):**\\n{formatted_meminfo}\\n\\n\"\n",
    "            f\"ðŸŒ **Network Devices (First 5 Lines):**\\n{formatted_netdev}\\n\\n\"\n",
    "            f\"â³ **Uptime:** {uptime} seconds\"\n",
    "        )\n",
    "\n",
    "kernel.add_plugin(ProcInfoPlugin(), \"proc_info_plugin\")\n",
    "\n",
    "class SaveTextFilePlugin:\n",
    "    @kernel_function(\n",
    "        name=\"save_text_file\",\n",
    "        description=\"When asked to write information to a file, use this function with the specified filename.\"\n",
    "    )\n",
    "    async def save_text_file(self, filename: str, text: str) -> str:\n",
    "        \"\"\"Asynchronously writes text content to a file.\"\"\"\n",
    "        def write_file():\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "        await asyncio.to_thread(write_file)\n",
    "        return f\"File '{filename}' saved successfully.\"\n",
    "\n",
    "kernel.add_plugin(SaveTextFilePlugin(), \"save_text_file_plugin\")\n",
    "\n",
    "# Configure the Ollama chat completion service\n",
    "model_name = \"llama3.2\"  # Ensure this model is pulled and available\n",
    "ollama_endpoint = \"http://localhost:11434\"\n",
    "chat_completion_service = OllamaChatCompletion(ai_model_id=model_name, host=ollama_endpoint)\n",
    "\n",
    "# Create request settings for Ollama\n",
    "request_settings = OllamaChatPromptExecutionSettings()\n",
    "request_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={\"excluded_plugins\": [\"ChatBot\"]})\n",
    "\n",
    "# Register the Ollama service with the kernel\n",
    "kernel.add_service(chat_completion_service)\n",
    "\n",
    "# User query\n",
    "#user_input = \"Write a summery information to a file about the operating system information to the file C:\\\\temp\\\\system_info.txt\"\n",
    "user_input = \"Get the operating system information and Write it to the file C:\\\\temp\\\\system_info.txt\"\n",
    "# Initialize chat history\n",
    "history = ChatHistory()\n",
    "history.add_user_message(user_input)\n",
    "\n",
    "# Update arguments with user input and chat history\n",
    "arguments = KernelArguments(settings=request_settings)\n",
    "arguments[\"user_input\"] = user_input\n",
    "arguments[\"chat_history\"] = history\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\")\n",
    "    \n",
    "# Invoke the chat function\n",
    "result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "\n",
    "# Process the result\n",
    "if result:\n",
    "    response = result.value[0]\n",
    "    print(f\"Chatbot:> {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Exercise**\n",
    "\n",
    "**Exercise 5: Building a Chat with your system Chatbot**\n",
    "\n",
    "### **1. Persistent Chatbot with History**\n",
    "- Modify the chatbot so it maintains a conversation history, allowing the user to ask follow-up questions.\n",
    "- Use **`ChatHistory`** from `semantic_kernel.contents` to store previous messages.\n",
    "\n",
    "### **2. Modular System Information Functions**\n",
    "Break down the **`read_proc_info`** function into multiple functions, each dedicated to specific system details:\n",
    "- **CPU Info:** Read `/proc/cpuinfo`\n",
    "- **Memory Info:** Read `/proc/meminfo`\n",
    "- **Disk Usage:** Use `df -h`\n",
    "- **Running Processes:** Read `/proc/[PID]/status`\n",
    "- **Network Info:** Parse `/proc/net/dev`\n",
    "\n",
    "### **3. Extended System Information**\n",
    "Extend the system information retrieval with:\n",
    "- **GPU Information:** Use `lspci`\n",
    "- **Mounted Drives:** Use `mount -v`\n",
    "- **Kernel Version & OS Info:** Use `uname -a` (Linux)\n",
    "\n",
    "### **4. Running Whitelisted Processes**\n",
    "- Introduce a **whitelist** of allowed processes.\n",
    "- Create a function `run_whitelisted_process(command: str)` that checks if the command is in the whitelist before execution.\n",
    "- Use `subprocess.run([...])` in Python, ensuring that user input is sanitized to avoid security risks.\n",
    "\n",
    "### **5. Killing Processes Started by the Chatbot**\n",
    "- Track processes that the chatbot starts.\n",
    "- Provide a function to terminate only those processes.\n",
    "- Use:\n",
    "  - `os.kill(pid, signal.SIGTERM)`\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation Plan**\n",
    "- Modify the chatbot to maintain history.\n",
    "- Refactor system information retrieval into multiple functions.\n",
    "- Implement process execution control using a **whitelist**.\n",
    "- Add a function to **terminate chatbot-created processes** safely.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Based on the output, your IP address is:\n",
      "\n",
      "* IPv4 Address: 192.168.0.50\n",
      "* Subnet Mask: 255.255.255.0\n",
      "* Default Gateway: fe80::ea9c:25ff:fe89:7f08%\n",
      "\n",
      "Your CPU information is:\n",
      "\n",
      "* Manufacturer: Intel\n",
      "* Model Name: Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import psutil\n",
    "import subprocess\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "# Initialize kernel and chatbot history\n",
    "kernel = Kernel()\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "# --- SYSTEM INFORMATION MODULE ---\n",
    "class SystemInfoPlugin:\n",
    "    @kernel_function(name=\"get_cpu_info\", description=\"Retrieve CPU information.\")\n",
    "    async def get_cpu_info(self) -> str:\n",
    "        return subprocess.run(\"lscpu\", capture_output=True, text=True).stdout if os.name != \"nt\" else subprocess.run(\"wmic cpu get Name\", capture_output=True, text=True).stdout\n",
    "\n",
    "    @kernel_function(name=\"get_memory_info\", description=\"Retrieve memory usage details.\")\n",
    "    async def get_memory_info(self) -> str:\n",
    "        return subprocess.run(\"free -h\", capture_output=True, text=True).stdout if os.name != \"nt\" else subprocess.run(\"wmic OS get FreePhysicalMemory,TotalVisibleMemorySize\", capture_output=True, text=True).stdout\n",
    "    \n",
    "    @kernel_function(name=\"get_disk_info\", description=\"Retrieve disk usage details.\")\n",
    "    async def get_disk_info(self) -> str:\n",
    "        return subprocess.run(\"df -h\", capture_output=True, text=True).stdout if os.name != \"nt\" else subprocess.run(\"wmic logicaldisk get size,freespace,caption\", capture_output=True, text=True).stdout\n",
    "    \n",
    "    @kernel_function(name=\"get_network_info\", description=\"Retrieve network interfaces and details.\")\n",
    "    async def get_network_info(self) -> str:\n",
    "        return subprocess.run(\"ip a\", capture_output=True, text=True).stdout if os.name != \"nt\" else subprocess.run(\"ipconfig /all\", capture_output=True, text=True).stdout\n",
    "\n",
    "kernel.add_plugin(SystemInfoPlugin(), \"system_info_plugin\")\n",
    "\n",
    "# --- PROCESS MANAGEMENT MODULE ---\n",
    "class ProcessManagerPlugin:\n",
    "    allowed_processes = {\"ping\", \"ls\", \"dir\", \"echo\", \"whoami\"}\n",
    "    running_processes = {}\n",
    "    \n",
    "    @kernel_function(name=\"run_whitelisted_process\", description=\"Run a whitelisted system process and return the output.\")\n",
    "    async def run_whitelisted_process(self, command: str) -> str:\n",
    "        command_name = command.split()[0]\n",
    "        if command_name not in self.allowed_processes:\n",
    "            return f\"Error: The command '{command_name}' is not allowed.\"\n",
    "        \n",
    "        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        self.running_processes[process.pid] = process\n",
    "        stdout, stderr = process.communicate()\n",
    "        output = stdout if stdout else stderr\n",
    "        return f\"Output of '{command}':\\n{output.strip()}\"\n",
    "    \n",
    "    @kernel_function(name=\"kill_process\", description=\"Terminate a process started by the chatbot.\")\n",
    "    async def kill_process(self, pid: int) -> str:\n",
    "        if pid in self.running_processes:\n",
    "            self.running_processes[pid].terminate()\n",
    "            del self.running_processes[pid]\n",
    "            return f\"Process {pid} terminated successfully.\"\n",
    "        return \"Error: Process ID not found or not started by chatbot.\"\n",
    "\n",
    "kernel.add_plugin(ProcessManagerPlugin(), \"process_manager_plugin\")\n",
    "\n",
    "# --- CHATBOT SETUP ---\n",
    "ollama_endpoint = \"http://localhost:11434\"\n",
    "model_name = \"llama3.2\"\n",
    "chat_service = OllamaChatCompletion(ai_model_id=model_name, host=ollama_endpoint)\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# Chat settings\n",
    "request_settings = OllamaChatPromptExecutionSettings()\n",
    "request_settings.function_choice_behavior = FunctionChoiceBehavior.Auto(filters={\"excluded_plugins\": [\"ChatBot\"]})\n",
    "\n",
    "# Define and register ChatBot plugin implementation\n",
    "class ChatBotPlugin:\n",
    "    @kernel_function(name=\"Chat\", description=\"Chat function that delegates to the Ollama chat service\")\n",
    "    async def Chat(self, chat_history, user_input) -> str:\n",
    "        result = await kernel.invoke_prompt(user_input)\n",
    "        return result.value[0] if result and result.value else \"No response received.\"\n",
    "\n",
    "kernel.add_plugin(ChatBotPlugin(), \"ChatBot\")\n",
    "\n",
    "chat_function = kernel.add_function(prompt=\"{{$chat_history}}{{$user_input}}\", plugin_name=\"ChatBot\", function_name=\"Chat\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    \n",
    "    chat_history.add_user_message(user_input)\n",
    "    arguments = KernelArguments(settings=request_settings)\n",
    "    arguments[\"user_input\"] = user_input\n",
    "    arguments[\"chat_history\"] = chat_history\n",
    "\n",
    "    result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "    if result:\n",
    "        response = result.value[0]\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        #chat_history.add_assistant_message(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up offline environment**\n",
    "\n",
    "To set up your environment for running the provided Jupyter Notebook in a disconnected setting, follow these steps:\n",
    "\n",
    "1. **Download and Prepare Dependencies**: Use the following script to download all necessary models and Docker images. This script should be executed in an environment with internet access.\n",
    "\n",
    "   ```bash\n",
    "   #!/bin/bash\n",
    "\n",
    "   # Create a directory to store all resources\n",
    "   mkdir -p llm_resources\n",
    "   cd llm_resources\n",
    "\n",
    "   # Download the LLaMA 2 model\n",
    "   git lfs install\n",
    "   git clone https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
    "   mv Llama-2-7B-Chat-GGUF models\n",
    "\n",
    "   # Pull the Ollama Docker image\n",
    "   docker pull ollama/ollama:latest\n",
    "   docker save ollama/ollama:latest -o ollama_latest.tar\n",
    "\n",
    "   # Create a requirements file for Python dependencies\n",
    "   cat <<EOF > requirements.txt\n",
    "   torch\n",
    "   torchvision\n",
    "   torchaudio\n",
    "   faiss-cpu\n",
    "   numpy\n",
    "   sentence-transformers\n",
    "   semantic-kernel\n",
    "   huggingface_hub\n",
    "   llama-cpp-python\n",
    "   EOF\n",
    "\n",
    "   # Download Python packages\n",
    "   pip download -r requirements.txt -d python_packages\n",
    "\n",
    "   echo \"All resources have been downloaded and saved in the 'llm_resources' directory.\"\n",
    "   ```\n",
    "\n",
    "\n",
    "   **Instructions**:\n",
    "\n",
    "   - Run the above script on a machine with internet access.\n",
    "   - Transfer the `llm_resources` directory to your target offline environment.\n",
    "\n",
    "2. **Set Up in the Disconnected Environment**:\n",
    "\n",
    "   - **Install Docker**: Ensure Docker is installed on your offline machine. If not, download the Docker installation package appropriate for your system and transfer it to the machine for installation.\n",
    "\n",
    "   - **Load the Ollama Docker Image**: Navigate to the `llm_resources` directory and load the Docker image:\n",
    "\n",
    "     ```bash\n",
    "     docker load -i ollama_latest.tar\n",
    "     ```\n",
    "\n",
    "   - **Install Python Dependencies**: Use the pre-downloaded Python packages to set up your environment:\n",
    "\n",
    "     ```bash\n",
    "     pip install --no-index --find-links=python_packages -r requirements.txt\n",
    "     ```\n",
    "\n",
    "   - **Set Up Models**: Ensure that the downloaded LLaMA 2 model is placed in the appropriate directory as expected by your Jupyter Notebook.\n",
    "\n",
    "3. **Running the Jupyter Notebook**:\n",
    "\n",
    "   - **Start the Ollama Server**: Run the Ollama server using Docker:\n",
    "\n",
    "     ```bash\n",
    "     docker run -d --name ollama -p 11434:11434 ollama/ollama:latest\n",
    "     ```\n",
    "\n",
    "   - **Launch Jupyter Notebook**: Navigate to your project directory and start Jupyter Notebook:\n",
    "\n",
    "     ```bash\n",
    "     jupyter notebook\n",
    "     ```\n",
    "\n",
    "   - **Access the Notebook**: Open your web browser and navigate to the Jupyter Notebook interface to open and run your notebook.\n",
    "\n",
    "By following these steps, you can set up and run your Jupyter Notebook in an environment without internet access. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
